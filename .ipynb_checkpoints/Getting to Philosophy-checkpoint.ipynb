{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widebot \n",
    "## Task 1 - Getting to Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - Getting to Philosophy\n",
    "\n",
    "Please write a Python script to check the \"Getting to Philosophy\" law.\n",
    "https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\n",
    "\n",
    "Clicking on the first link in the main body of a Wikipedia article and repeating the process for subsequent articles would usually lead to the article Philosophy.\n",
    "\n",
    "The program should receive a Wikipedia link as an input, go to another normal link and repeat this process until either Philosophy page is reached, or we are in an article without any outgoing Wikilinks, or stuck in a loop.\n",
    "\n",
    "A \"normal link\" is a link from the main page article, not in a box, is blue (red is for non-existing articles), not in parentheses, not italic and not a footnote. You don't have to check style tables or other fancy things, it is enough that the script works with the current Wikipedia style (for example you can use 'class' attribute in Wikipedia tags). For easy validation, please print all visited links to the standard output.\n",
    "\n",
    "Use a 0.5 second timeout between queries to avoid heavy load on Wikipedia (sleep function from time module).\n",
    "\n",
    "You can use https://en.wikipedia.org/wiki/Special:Random to check this hypothesis at home.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Philosophy will stop when : \n",
    "1. reaches the Philosophy article\n",
    "2. reaches 30 article without reaching the Philosophy article \n",
    "3. get into a article with no links \n",
    "4. stuck in a loop (like mathematics article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n",
    "start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "# target\n",
    "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "# store the visited article \n",
    "visited_urls = [start_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_link(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # This div stars with the body of the article\n",
    "    content_div = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "\n",
    "    # if the link contains no links it remains None\n",
    "    article_link = None\n",
    "\n",
    "    # Find all the direct children of content_div that are paragraphs\n",
    "    for element in content_div.find_all(\"p\", recursive=False):\n",
    "        #find only the direct children\n",
    "        if element.find(\"a\", recursive=False):\n",
    "            article_link = element.find(\"a\", recursive=False).get('href')\n",
    "            break\n",
    "\n",
    "    if not article_link:\n",
    "        return\n",
    "\n",
    "    # Build a full url \n",
    "    first_link = urllib.parse.urljoin(\n",
    "        'https://en.wikipedia.org/', article_link)\n",
    "\n",
    "    return first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_scraping(scraping_history, target_url, max_steps=30):\n",
    "    # When reaches to philosphy\n",
    "    if scraping_history[-1] == target_url:\n",
    "        print(\"Target ('Philosphy') article reached!\")\n",
    "        return False\n",
    "    # max iterations \n",
    "    elif len(scraping_history) > max_steps:\n",
    "        print(\"Maximum (30) searches reached, interrupted.\")\n",
    "        return False\n",
    "    elif scraping_history[-1] in scraping_history[:-1]:\n",
    "        print(\"We are in a Loop , interrupted.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://en.wikipedia.org/wiki/Finnish_language\n",
      "https://en.wikipedia.org/wiki/Finnic_language\n",
      "https://en.wikipedia.org/wiki/Uralic_language_family\n",
      "https://en.wikipedia.org/wiki/Language_family\n",
      "https://en.wikipedia.org/wiki/Language\n",
      "https://en.wikipedia.org/wiki/Linguistic_system\n",
      "https://en.wikipedia.org/wiki/Ferdinand_de_Saussure\n",
      "https://en.wikipedia.org/wiki/Switzerland\n",
      "https://en.wikipedia.org/wiki/Sovereign_state\n",
      "https://en.wikipedia.org/wiki/International_law\n",
      "https://en.wikipedia.org/wiki/Nation\n",
      "https://en.wikipedia.org/wiki/Community\n",
      "https://en.wikipedia.org/wiki/Norm_(social)\n",
      "https://en.wikipedia.org/wiki/Sociology\n",
      "https://en.wikipedia.org/wiki/Society\n",
      "https://en.wikipedia.org/wiki/Social_group\n",
      "https://en.wikipedia.org/wiki/Social_science\n",
      "https://en.wikipedia.org/wiki/Discipline_(academia)\n",
      "https://en.wikipedia.org/wiki/Knowledge\n",
      "https://en.wikipedia.org/wiki/Fact\n",
      "https://en.wikipedia.org/wiki/Reality\n",
      "https://en.wikipedia.org/wiki/Object_of_the_mind\n",
      "https://en.wikipedia.org/wiki/Object_(philosophy)\n",
      "Target ('Philosphy') article reached!\n"
     ]
    }
   ],
   "source": [
    "while continue_scraping(visited_urls, target_url):\n",
    "    #print first link\n",
    "    print(visited_urls[-1])\n",
    "\n",
    "    first_link = find_first_link(visited_urls[-1])\n",
    "    # when arrive at an article with no links\n",
    "    if not first_link:\n",
    "        print(\"Arrived at an article with no links, search aborted.\")\n",
    "        break\n",
    "\n",
    "    visited_urls.append(first_link)\n",
    "\n",
    "    time.sleep(0.5)  # Slow things down so as to not overload Wikipedia's servers\n",
    "visited_urls=[start_url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mohamed Adel Mohamed \n",
    "### Mohamedadelmohamed@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
